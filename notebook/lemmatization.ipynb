{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "## Anish Sachdeva (DTU/2K16/MC/013)\n",
    "\n",
    "## Natural Language Processing (NLP) - Dr. Seba Susan\n",
    "\n",
    "### Overview\n",
    "We introduce lemmatization in this Notebook and alsous lemmatization to process our resume to lemmatize each toke i the corpus (Resume). We also compare the results with previously seen Porter Stemmer algorithm on our Resume.\n",
    "\n",
    "### 1. Introduction\n",
    "\n",
    "For grammatical reasons, documents are going to use different forms of a word, such as organize, organizes, and organizing. Additionally, there are families of derivationally related words with similar meanings, such as democracy, democratic, and democratization. In many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set.\n",
    "\n",
    "The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. For instance:\n",
    "\n",
    "\n",
    "_am, are, is_ $\\Rightarrow$ __be__\n",
    "\n",
    "_car, cars, car's, cars'_ $\\Rightarrow$ __car__\n",
    "\n",
    "The Result of such mappings can b something like this:\n",
    "\n",
    "_the boy's cars are different colors_ $\\Rightarrow$ _the boy car be differ color_\n",
    "\n",
    "However, the two words differ in their flavor. Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma . \n",
    "\n",
    "If confronted with the token _saw_, stemming might return just _s_, whereas lemmatization would attempt to return either _see_ or _saw_ depending on whether the use of the token was as a verb or a noun. The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatization commonly only collapses the different inflectional forms of a lemma. \n",
    "\n",
    "Linguistic processing for stemming or lemmatization is often done by an additional plug-in component to the indexing process, and a number of such components exist, both commercial and open-source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing basic tools required for tokenization, stemming and lemmatization of the resume corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\anish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenizer\n",
    "The Tokenizer is a basic tool used in NLP that returns tokens from a given document. We can create our own token as well based on Regex. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self._tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    def tokenize(self, document: str) -> list:\n",
    "        return self._tokenizer.tokenize(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Have', 'no', 'fear', 'of', 'perfection', 'You', 'll', 'never', 'reach', 'it']\n"
     ]
    }
   ],
   "source": [
    "# We define a basic tokenizer that takes all word strings from a document as a token. Test it below\n",
    "# You can also modify the sentence below to see tokens\n",
    "message = \"Have no fear of perfection. You'll never reach it ðŸ”¥\"\n",
    "tokenizer = Tokenizer()\n",
    "print(tokenizer.tokenize(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Porter Stemmer\n",
    "We now define the Porter Stemmer Algorithm, one of the most famous stemming algorithm for the english language created by M. F. Porter in 1980. This algorithm reduces a token by removing it's inflections. e.g. _running_ $\\rightarrow$ _run_.\n",
    "\n",
    "We use the porter stemmer as defined in the first assignment which can be seen [here](https://github.com/anishLearnsToCode/porter-stemmer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PorterStemmer:\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"The word is a buffer holding a word to be stemmed. The letters are in the range\n",
    "        [start, offset ... offset + 1) ... ending at end.\"\"\"\n",
    "\n",
    "        self.vowels = ('a', 'e', 'i', 'o', 'u')\n",
    "        self.word = ''\n",
    "        self.end = 0\n",
    "        self.start = 0\n",
    "        self.offset = 0\n",
    "        self._tokenizer = Tokenizer()\n",
    "\n",
    "    def is_vowel(self, letter):\n",
    "        return letter in self.vowels\n",
    "\n",
    "    def is_consonant(self, index):\n",
    "        \"\"\":returns True if word[index] is a consonant.\"\"\"\n",
    "        if self.is_vowel(self.word[index]):\n",
    "            return False\n",
    "        if self.word[index] == 'y':\n",
    "            if index == self.start:\n",
    "                return True\n",
    "            else:\n",
    "                return not self.is_consonant(index - 1)\n",
    "        return True\n",
    "\n",
    "    def m(self):\n",
    "        \"\"\"m() measures the number of consonant sequences between start and offset.\n",
    "        if c is a consonant sequence and v a vowel sequence, and <..>\n",
    "        indicates arbitrary presence,\n",
    "           <c><v>       gives 0\n",
    "           <c>vc<v>     gives 1\n",
    "           <c>vcvc<v>   gives 2\n",
    "           <c>vcvcvc<v> gives 3\n",
    "           ....\n",
    "        \"\"\"\n",
    "        n = 0\n",
    "        i = self.start\n",
    "        while True:\n",
    "            if i > self.offset:\n",
    "                return n\n",
    "            if not self.is_consonant(i):\n",
    "                break\n",
    "            i += 1\n",
    "        i += 1\n",
    "        while True:\n",
    "            while True:\n",
    "                if i > self.offset:\n",
    "                    return n\n",
    "                if self.is_consonant(i):\n",
    "                    break\n",
    "                i += 1\n",
    "            i += 1\n",
    "            n += 1\n",
    "            while True:\n",
    "                if i > self.offset:\n",
    "                    return n\n",
    "                if not self.is_consonant(i):\n",
    "                    break\n",
    "                i += 1\n",
    "            i += 1\n",
    "\n",
    "    def contains_vowel(self):\n",
    "        \"\"\":returns TRUE if the word contains a vowel in the range [start, offset]\"\"\"\n",
    "        for i in range(self.start, self.offset + 1):\n",
    "            if not self.is_consonant(i):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def contains_double_consonant(self, j):\n",
    "        \"\"\":returns TRUE if the word contain a double consonant in the range [offset, start]\"\"\"\n",
    "        if j < (self.start + 1):\n",
    "            return False\n",
    "        if self.word[j] != self.word[j - 1]:\n",
    "            return False\n",
    "        return self.is_consonant(j)\n",
    "\n",
    "    def is_of_form_cvc(self, i):\n",
    "        \"\"\":returns TRUE for indices set {i-2, i-1, i} has the form consonant - vowel - consonant\n",
    "        and also if the second c is not w,x or y. this is used when trying to\n",
    "        restore an e at the end of a short  e.g.\n",
    "           cav(e), lov(e), hop(e), crim(e), but\n",
    "           snow, box, tray.\n",
    "        \"\"\"\n",
    "        if i < (self.start + 2) or not self.is_consonant(i) or self.is_consonant(i - 1) or not self.is_consonant(i - 2):\n",
    "            return 0\n",
    "        ch = self.word[i]\n",
    "        if ch == 'w' or ch == 'x' or ch == 'y':\n",
    "            return 0\n",
    "        return 1\n",
    "\n",
    "    def ends_with(self, s):\n",
    "        \"\"\":returns TRUE when {start...end} ends with the string s.\"\"\"\n",
    "        length = len(s)\n",
    "        if s[length - 1] != self.word[self.end]:  # tiny speed-up\n",
    "            return False\n",
    "        if length > (self.end - self.start + 1):\n",
    "            return False\n",
    "        if self.word[self.end - length + 1: self.end + 1] != s:\n",
    "            return False\n",
    "        self.offset = self.end - length\n",
    "        return True\n",
    "\n",
    "    def set_to(self, s):\n",
    "        \"\"\"sets [offset + 1, end] to the characters in the string s, readjusting end.\"\"\"\n",
    "        length = len(s)\n",
    "        self.word = self.word[:self.offset + 1] + s + self.word[self.offset + length + 1:]\n",
    "        self.end = self.offset + length\n",
    "\n",
    "    def replace_morpheme(self, s):\n",
    "        \"\"\"is a mapping function to change morphemes\"\"\"\n",
    "        if self.m() > 0:\n",
    "            self.set_to(s)\n",
    "\n",
    "    def remove_plurals(self):\n",
    "        \"\"\"This is step 1 ab and gets rid of plurals and -ed or -ing. e.g.\n",
    "           caresses  ->  caress\n",
    "           ponies    ->  poni\n",
    "           ties      ->  ti\n",
    "           caress    ->  caress\n",
    "           cats      ->  cat\n",
    "           feed      ->  feed\n",
    "           agreed    ->  agree\n",
    "           disabled  ->  disable\n",
    "           matting   ->  mat\n",
    "           mating    ->  mate\n",
    "           meeting   ->  meet\n",
    "           milling   ->  mill\n",
    "           messing   ->  mess\n",
    "           meetings  ->  meet\n",
    "        \"\"\"\n",
    "        if self.word[self.end] == 's':\n",
    "            if self.ends_with(\"sses\"):\n",
    "                self.end = self.end - 2\n",
    "            elif self.ends_with(\"ies\"):\n",
    "                self.set_to(\"i\")\n",
    "            elif self.word[self.end - 1] != 's':\n",
    "                self.end = self.end - 1\n",
    "        if self.ends_with(\"eed\"):\n",
    "            if self.m() > 0:\n",
    "                self.end = self.end - 1\n",
    "        elif (self.ends_with(\"ed\") or self.ends_with(\"ing\")) and self.contains_vowel():\n",
    "            self.end = self.offset\n",
    "            if self.ends_with(\"at\"):\n",
    "                self.set_to(\"ate\")\n",
    "            elif self.ends_with(\"bl\"):\n",
    "                self.set_to(\"ble\")\n",
    "            elif self.ends_with(\"iz\"):\n",
    "                self.set_to(\"ize\")\n",
    "            elif self.contains_double_consonant(self.end):\n",
    "                self.end = self.end - 1\n",
    "                ch = self.word[self.end]\n",
    "                if ch == 'l' or ch == 's' or ch == 'z':\n",
    "                    self.end = self.end + 1\n",
    "            elif self.m() == 1 and self.is_of_form_cvc(self.end):\n",
    "                self.set_to(\"e\")\n",
    "\n",
    "    def terminal_y_to_i(self):\n",
    "        \"\"\"This defines step 1 c which turns terminal y to i when there is another vowel in the stem.\"\"\"\n",
    "        if self.ends_with('y') and self.contains_vowel():\n",
    "            self.word = self.word[:self.end] + 'i' + self.word[self.end + 1:]\n",
    "\n",
    "    def map_double_to_single_suffix(self):\n",
    "        \"\"\"Defines step 2 and maps double suffices to single ones.\n",
    "        so -ization ( = -ize plus -ation) maps to -ize etc. note that the\n",
    "        string before the suffix must give m() > 0.\n",
    "        \"\"\"\n",
    "        if self.word[self.end - 1] == 'a':\n",
    "            if self.ends_with(\"ational\"):\n",
    "                self.replace_morpheme(\"ate\")\n",
    "            elif self.ends_with(\"tional\"):\n",
    "                self.replace_morpheme(\"tion\")\n",
    "        elif self.word[self.end - 1] == 'c':\n",
    "            if self.ends_with(\"enci\"):\n",
    "                self.replace_morpheme(\"ence\")\n",
    "            elif self.ends_with(\"anci\"):\n",
    "                self.replace_morpheme(\"ance\")\n",
    "        elif self.word[self.end - 1] == 'e':\n",
    "            if self.ends_with(\"izer\"):      self.replace_morpheme(\"ize\")\n",
    "        elif self.word[self.end - 1] == 'l':\n",
    "            if self.ends_with(\"bli\"):\n",
    "                self.replace_morpheme(\"ble\")  # --DEPARTURE--\n",
    "            # To match the published algorithm, replace this phrase with\n",
    "            #   if self.ends(\"abli\"):      self.r(\"able\")\n",
    "            elif self.ends_with(\"alli\"):\n",
    "                self.replace_morpheme(\"al\")\n",
    "            elif self.ends_with(\"entli\"):\n",
    "                self.replace_morpheme(\"ent\")\n",
    "            elif self.ends_with(\"eli\"):\n",
    "                self.replace_morpheme(\"e\")\n",
    "            elif self.ends_with(\"ousli\"):\n",
    "                self.replace_morpheme(\"ous\")\n",
    "        elif self.word[self.end - 1] == 'o':\n",
    "            if self.ends_with(\"ization\"):\n",
    "                self.replace_morpheme(\"ize\")\n",
    "            elif self.ends_with(\"ation\"):\n",
    "                self.replace_morpheme(\"ate\")\n",
    "            elif self.ends_with(\"ator\"):\n",
    "                self.replace_morpheme(\"ate\")\n",
    "        elif self.word[self.end - 1] == 's':\n",
    "            if self.ends_with(\"alism\"):\n",
    "                self.replace_morpheme(\"al\")\n",
    "            elif self.ends_with(\"iveness\"):\n",
    "                self.replace_morpheme(\"ive\")\n",
    "            elif self.ends_with(\"fulness\"):\n",
    "                self.replace_morpheme(\"ful\")\n",
    "            elif self.ends_with(\"ousness\"):\n",
    "                self.replace_morpheme(\"ous\")\n",
    "        elif self.word[self.end - 1] == 't':\n",
    "            if self.ends_with(\"aliti\"):\n",
    "                self.replace_morpheme(\"al\")\n",
    "            elif self.ends_with(\"iviti\"):\n",
    "                self.replace_morpheme(\"ive\")\n",
    "            elif self.ends_with(\"biliti\"):\n",
    "                self.replace_morpheme(\"ble\")\n",
    "        elif self.word[self.end - 1] == 'g':\n",
    "            if self.ends_with(\"logi\"):      self.replace_morpheme(\"log\")\n",
    "\n",
    "    def step3(self):\n",
    "        \"\"\"step3() deals with -ic-, -full, -ness etc.\"\"\"\n",
    "        if self.word[self.end] == 'e':\n",
    "            if self.ends_with(\"icate\"):\n",
    "                self.replace_morpheme(\"ic\")\n",
    "            elif self.ends_with(\"ative\"):\n",
    "                self.replace_morpheme(\"\")\n",
    "            elif self.ends_with(\"alize\"):\n",
    "                self.replace_morpheme(\"al\")\n",
    "        elif self.word[self.end] == 'i':\n",
    "            if self.ends_with(\"iciti\"):     self.replace_morpheme(\"ic\")\n",
    "        elif self.word[self.end] == 'l':\n",
    "            if self.ends_with(\"ical\"):\n",
    "                self.replace_morpheme(\"ic\")\n",
    "            elif self.ends_with(\"ful\"):\n",
    "                self.replace_morpheme(\"\")\n",
    "        elif self.word[self.end] == 's':\n",
    "            if self.ends_with(\"ness\"):      self.replace_morpheme(\"\")\n",
    "\n",
    "    def step4(self):\n",
    "        \"\"\"step4() takes off -ant, -ence etc., in context <c>vcvc<v>.\"\"\"\n",
    "        if self.word[self.end - 1] == 'a':\n",
    "            if self.ends_with(\"al\"):\n",
    "                pass\n",
    "            else:\n",
    "                return\n",
    "        elif self.word[self.end - 1] == 'c':\n",
    "            if self.ends_with(\"ance\"):\n",
    "                pass\n",
    "            elif self.ends_with(\"ence\"):\n",
    "                pass\n",
    "            else:\n",
    "                return\n",
    "        elif self.word[self.end - 1] == 'e':\n",
    "            if self.ends_with(\"er\"):\n",
    "                pass\n",
    "            else:\n",
    "                return\n",
    "        elif self.word[self.end - 1] == 'i':\n",
    "            if self.ends_with(\"ic\"):\n",
    "                pass\n",
    "            else:\n",
    "                return\n",
    "        elif self.word[self.end - 1] == 'l':\n",
    "            if self.ends_with(\"able\"):\n",
    "                pass\n",
    "            elif self.ends_with(\"ible\"):\n",
    "                pass\n",
    "            else:\n",
    "                return\n",
    "        elif self.word[self.end - 1] == 'n':\n",
    "            if self.ends_with(\"ant\"):\n",
    "                pass\n",
    "            elif self.ends_with(\"ement\"):\n",
    "                pass\n",
    "            elif self.ends_with(\"ment\"):\n",
    "                pass\n",
    "            elif self.ends_with(\"ent\"):\n",
    "                pass\n",
    "            else:\n",
    "                return\n",
    "        elif self.word[self.end - 1] == 'o':\n",
    "            if self.ends_with(\"ion\") and (self.word[self.offset] == 's' or self.word[self.offset] == 't'):\n",
    "                pass\n",
    "            elif self.ends_with(\"ou\"):\n",
    "                pass\n",
    "            # takes care of -ous\n",
    "            else:\n",
    "                return\n",
    "        elif self.word[self.end - 1] == 's':\n",
    "            if self.ends_with(\"ism\"):\n",
    "                pass\n",
    "            else:\n",
    "                return\n",
    "        elif self.word[self.end - 1] == 't':\n",
    "            if self.ends_with(\"ate\"):\n",
    "                pass\n",
    "            elif self.ends_with(\"iti\"):\n",
    "                pass\n",
    "            else:\n",
    "                return\n",
    "        elif self.word[self.end - 1] == 'u':\n",
    "            if self.ends_with(\"ous\"):\n",
    "                pass\n",
    "            else:\n",
    "                return\n",
    "        elif self.word[self.end - 1] == 'v':\n",
    "            if self.ends_with(\"ive\"):\n",
    "                pass\n",
    "            else:\n",
    "                return\n",
    "        elif self.word[self.end - 1] == 'z':\n",
    "            if self.ends_with(\"ize\"):\n",
    "                pass\n",
    "            else:\n",
    "                return\n",
    "        else:\n",
    "            return\n",
    "        if self.m() > 1:\n",
    "            self.end = self.offset\n",
    "\n",
    "    def step5(self):\n",
    "        \"\"\"step5() removes a final -e if m() > 1, and changes -ll to -l if m > 1.\"\"\"\n",
    "        self.offset = self.end\n",
    "        if self.word[self.end] == 'e':\n",
    "            a = self.m()\n",
    "            if a > 1 or (a == 1 and not self.is_of_form_cvc(self.end - 1)):\n",
    "                self.end = self.end - 1\n",
    "        if self.word[self.end] == 'l' and self.contains_double_consonant(self.end) and self.m() > 1:\n",
    "            self.end = self.end - 1\n",
    "\n",
    "    def stem_document(self, document):\n",
    "        result = []\n",
    "        for line in document.split('\\n'):\n",
    "            result.append(self.stem_sentence(line))\n",
    "        return '\\n'.join(result)\n",
    "\n",
    "    def alphabetic(self, word):\n",
    "        return ''.join([letter if letter.isalpha() else '' for letter in word])\n",
    "\n",
    "    def stem_sentence(self, sentence):\n",
    "        result = []\n",
    "        for word in self._tokenizer.tokenize(sentence):\n",
    "            result.append(self.stem_word(word))\n",
    "        return ' '.join(result)\n",
    "\n",
    "    def stem_word(self, word):\n",
    "        if word == '':\n",
    "            return ''\n",
    "\n",
    "        self.word = word\n",
    "        self.end = len(word) - 1\n",
    "        self.start = 0\n",
    "\n",
    "        self.remove_plurals()\n",
    "        self.terminal_y_to_i()\n",
    "        self.map_double_to_single_suffix()\n",
    "        self.step3()\n",
    "        self.step4()\n",
    "        self.step5()\n",
    "        return self.word[self.start: self.end + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n"
     ]
    }
   ],
   "source": [
    "# test the porter stemmer with any word of your choice\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem_word('running'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the boi s car ar differ color\n"
     ]
    }
   ],
   "source": [
    "# test the porter stemmer with any sentence of your choice\n",
    "print(stemmer.stem_sentence(\"the boy's cars are different colors\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Lemmatization\n",
    "We now create a class for lemmatization that will use the `Tokenizer` class. An instance of the Lemmatization class will help us reduce words into their __lemmas__.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lemmatizer:\n",
    "    def __init__(self):\n",
    "        self._lemmatizer = WordNetLemmatizer()\n",
    "        self._tokenizer = Tokenizer()\n",
    "\n",
    "    def _tokenize(self, document: str) -> list:\n",
    "        return self._tokenizer.tokenize(document)\n",
    "\n",
    "    def lemmatize_word(self, word: str, pos=None) -> str:\n",
    "        return self._lemmatizer.lemmatize(word, pos) if pos is not None else self._lemmatizer.lemmatize(word)\n",
    "\n",
    "    def lemmatize_sentence(self, sentence: str, pos=None) -> str:\n",
    "        result = []\n",
    "        for word in self._tokenize(sentence):\n",
    "            if pos is not None:\n",
    "                result.append(self.lemmatize_word(word, pos))\n",
    "            else:\n",
    "                result.append(self.lemmatize_word(word))\n",
    "        return ' '.join(result)\n",
    "\n",
    "    def lemmatize_document(self, document: str) -> str:\n",
    "        result = []\n",
    "        for line in document.split('\\n'):\n",
    "            result.append(self.lemmatize_sentence(line))\n",
    "        return '\\n'.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wolf\n"
     ]
    }
   ],
   "source": [
    "# test the lemmatizer with any word of your choice\n",
    "lemmatizer = Lemmatizer()\n",
    "print(lemmatizer.lemmatize_word('wolves'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the quick brown fox jump over the lazy dog\n"
     ]
    }
   ],
   "source": [
    "# try the lemmaztization algorithm with a sentence of your choice\n",
    "print(lemmatizer.lemmatize_sentence('the quick brown fox ðŸ¦Š jumps over the lazy dog ðŸ¶'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Loading in the Resume\n",
    "We now load in our resume and see the sample text in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anish sachdeva\n",
      "software developer + clean code enthusiast\n",
      "\n",
      "phone : 8287428181\n",
      "email : anish_@outlook.com\n",
      "home : sandesh vihar, pitampura, new delhi - 110034\n",
      "date of birth : 7th april 1998\n",
      "languages : english, hindi, french\n",
      "\n",
      "work experience\n",
      "what after college ( 4 months )\n",
      "delhi, india\n",
      "creating content to teach core java and python with data structures and algorithms and giving online classes to students\n",
      "\n",
      "summer research fellow at university of auckland ( 2 months )\n",
      "auckland, new zealand\n",
      "worked on geometry of mobius transformations, differential grometry under dr. pedram hekmati at the department of\n",
      "mathematics, university of auckland\n",
      "\n",
      "software developer at cern ( 14 months )\n",
      "cern, geneva, switzerland\n",
      "worked in the core platforms team of the fap-bc group. part of an agile team of developers that maintains and adds core\n",
      "functionality to applications used internally at cern by hr, financial, administrative and other departments\n",
      "including scientific\n",
      "\n",
      "worked on legacy applications that comprise of single and some times multiple frameworks such as java spring, boot,\n",
      "hibernate and java ee. also worked with google polymer 1.0 and jsp on the client side\n",
      "\n",
      "maintained cern's electronic document handing system application with >1m loc that comprising of multiple frameworks\n",
      "and created ~20 years ago. worked on feature requests, support requests and incidents and also release cycles\n",
      "\n",
      "teaching assistant ( 4 months )\n",
      "coding ninjas, delhi\n",
      "served as the teaching assistant to nucleus - java with ds batch, under mr. ankur kumar. worked on creating course\n",
      "content and quizzes for online platform of coding ninjas for java. helped students in core data structures and\n",
      "algorithms concepts in java\n",
      "\n",
      "education\n",
      "delhi technological university (2016 - 2021)\n",
      "bachelors of technology mathematics and computing\n",
      "cgpa: 9.2\n",
      "\n",
      "the heritage school rohini (2004 - 2016)\n",
      "physics, chemistry, maths + computer science with english\n",
      "senior secondary: 94.8%\n",
      "secondary: 9.8 cgpa\n",
      "\n",
      "technical skills\n",
      "java + algorithms and data structures\n",
      "mean stack web development\n",
      "python + machine learning\n",
      "matlab + octave\n",
      "mysql, postgressql & mongodb\n",
      "\n",
      "other skills\n",
      "ms office, adobe photoshop, latex + mitex\n",
      "\n",
      "university courses\n",
      "applied mathematics i, ii, iii\n",
      "linear algebra + probability & statistics + stochastic processes + discrete maths\n",
      "computer organization & architecture + data structures + algorithm design and analysis + dbms + os\n",
      "computer vision + nlp\n",
      "\n",
      "important links\n",
      "https://www.linkedin.com/in/anishsachdeva1998/\n",
      "https://github.com/anishlearnstocode\n",
      "https://www.hackerrank.com/anishviewer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resume_file = open('../assets/resume.txt', 'r')\n",
    "resume = resume_file.read().lower()\n",
    "resume_file.close()\n",
    "print(resume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Stemming the Resume\n",
    "We now Stem our resume by applying the PorterStemmer algorithm on it and see the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "resume_file = open('../assets/resume.txt', 'r')\n",
    "resume = resume_file.read().lower()\n",
    "resume_file.close()\n",
    "\n",
    "resume_stemmed = stemmer.stem_document(resume)\n",
    "pickle.dump(obj=resume_stemmed, file=open('../assets/resume_stemmed.p', 'wb'))\n",
    "\n",
    "resume_stemmed_file = open('../assets/resume_stemmed.txt', 'w')\n",
    "resume_stemmed_file.write(resume_stemmed)\n",
    "resume_stemmed_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anish sachdeva\n",
      "softwar develop clean code enthusiast\n",
      "\n",
      "phone 8287428181\n",
      "email anish_ outlook com\n",
      "home sandesh vihar pitampura new delhi 110034\n",
      "date of birth 7th april 1998\n",
      "languag english hindi french\n",
      "\n",
      "work experi\n",
      "what after colleg 4 month\n",
      "delhi india\n",
      "creat content to teach core java and python with data structur and algorithm and give onlin class to student\n",
      "\n",
      "summer research fellow at univers of auckland 2 month\n",
      "auckland new zealand\n",
      "work on geometri of mobiu transform differenti grometri under dr pedram hekmati at the depart of\n",
      "mathemat univers of auckland\n",
      "\n",
      "softwar develop at cern 14 month\n",
      "cern geneva switzerland\n",
      "work in the core platform team of the fap bc group part of an agil team of develop that maintain and add core\n",
      "function to applic us intern at cern by hr financi administr and other depart\n",
      "includ scientif\n",
      "\n",
      "work on legaci applic that compris of singl and some time multipl framework such a java spring boot\n",
      "hibern and java ee also work with googl polym 1 0 and jsp on the client side\n",
      "\n",
      "maintain cern s electron document hand system applic with 1m loc that compris of multipl framework\n",
      "and creat 20 year ago work on featur request support request and incid and also releas cycl\n",
      "\n",
      "teach assist 4 month\n",
      "code ninja delhi\n",
      "serv a the teach assist to nucleu java with d batch under mr ankur kumar work on creat cours\n",
      "content and quizz for onlin platform of code ninja for java help student in core data structur and\n",
      "algorithm concept in java\n",
      "\n",
      "educ\n",
      "delhi technolog univers 2016 2021\n",
      "bachelor of technolog mathemat and comput\n",
      "cgpa 9 2\n",
      "\n",
      "the heritag school rohini 2004 2016\n",
      "physic chemistri math comput scienc with english\n",
      "senior secondari 94 8\n",
      "secondari 9 8 cgpa\n",
      "\n",
      "technic skill\n",
      "java algorithm and data structur\n",
      "mean stack web develop\n",
      "python machin learn\n",
      "matlab octav\n",
      "mysql postgressql mongodb\n",
      "\n",
      "other skill\n",
      "m offic adob photoshop latex mitex\n",
      "\n",
      "univers cours\n",
      "appli mathemat i ii iii\n",
      "linear algebra probabl statist stochast process discret math\n",
      "comput organ architectur data structur algorithm design and analysi dbm o\n",
      "comput vision nlp\n",
      "\n",
      "import link\n",
      "http www linkedin com in anishsachdeva1998\n",
      "http github com anishlearnstocod\n",
      "http www hackerrank com anishview\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We now display the stemmed resume\n",
    "print(resume_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Creating the Lemmatized Resume\n",
    "We now use our `Lemmatizer` class to lemmatize our resume and save it so that we can run analytics on it later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = Lemmatizer()\n",
    "resume_file = open('../assets/resume.txt')\n",
    "resume = resume_file.read().lower()\n",
    "resume_file.close()\n",
    "\n",
    "resume_lemmatized = lemmatizer.lemmatize_document(resume)\n",
    "pickle.dump(resume_lemmatized, open('../assets/resume_lemmatized.p', 'wb'))\n",
    "\n",
    "resume_lemmatized_file = open('../assets/resume_lemmatized.txt', 'w')\n",
    "resume_lemmatized_file.write(resume_lemmatized)\n",
    "resume_lemmatized_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anish sachdeva\n",
      "software developer clean code enthusiast\n",
      "\n",
      "phone 8287428181\n",
      "email anish_ outlook com\n",
      "home sandesh vihar pitampura new delhi 110034\n",
      "date of birth 7th april 1998\n",
      "language english hindi french\n",
      "\n",
      "work experience\n",
      "what after college 4 month\n",
      "delhi india\n",
      "creating content to teach core java and python with data structure and algorithm and giving online class to student\n",
      "\n",
      "summer research fellow at university of auckland 2 month\n",
      "auckland new zealand\n",
      "worked on geometry of mobius transformation differential grometry under dr pedram hekmati at the department of\n",
      "mathematics university of auckland\n",
      "\n",
      "software developer at cern 14 month\n",
      "cern geneva switzerland\n",
      "worked in the core platform team of the fap bc group part of an agile team of developer that maintains and add core\n",
      "functionality to application used internally at cern by hr financial administrative and other department\n",
      "including scientific\n",
      "\n",
      "worked on legacy application that comprise of single and some time multiple framework such a java spring boot\n",
      "hibernate and java ee also worked with google polymer 1 0 and jsp on the client side\n",
      "\n",
      "maintained cern s electronic document handing system application with 1m loc that comprising of multiple framework\n",
      "and created 20 year ago worked on feature request support request and incident and also release cycle\n",
      "\n",
      "teaching assistant 4 month\n",
      "coding ninja delhi\n",
      "served a the teaching assistant to nucleus java with d batch under mr ankur kumar worked on creating course\n",
      "content and quiz for online platform of coding ninja for java helped student in core data structure and\n",
      "algorithm concept in java\n",
      "\n",
      "education\n",
      "delhi technological university 2016 2021\n",
      "bachelor of technology mathematics and computing\n",
      "cgpa 9 2\n",
      "\n",
      "the heritage school rohini 2004 2016\n",
      "physic chemistry math computer science with english\n",
      "senior secondary 94 8\n",
      "secondary 9 8 cgpa\n",
      "\n",
      "technical skill\n",
      "java algorithm and data structure\n",
      "mean stack web development\n",
      "python machine learning\n",
      "matlab octave\n",
      "mysql postgressql mongodb\n",
      "\n",
      "other skill\n",
      "m office adobe photoshop latex mitex\n",
      "\n",
      "university course\n",
      "applied mathematics i ii iii\n",
      "linear algebra probability statistic stochastic process discrete math\n",
      "computer organization architecture data structure algorithm design and analysis dbms o\n",
      "computer vision nlp\n",
      "\n",
      "important link\n",
      "http www linkedin com in anishsachdeva1998\n",
      "http github com anishlearnstocode\n",
      "http www hackerrank com anishviewer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# displaying lemmatized resume\n",
    "print(resume_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Analytics\n",
    "We now run a few basic analytics and compae the output of the stemmed and lemmaztized Resumes with each other and the original resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load in the original, stemmed and lemmatized resumes\n",
    "resume_file = open('../assets/resume.txt', 'r')\n",
    "resume = resume_file.read().lower()\n",
    "resume_file.close()\n",
    "resume_stemmed = pickle.load(open('../assets/resume_stemmed.p', 'rb'))\n",
    "resume_lemmatized = pickle.load(open('../assets/resume_lemmatized.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting tokens from the original, stemmed and lemmatized outputs\n",
    "resume_tokens = word_tokenize(resume)\n",
    "stemmed_resume_tokens = word_tokenize(resume_stemmed)\n",
    "lemmatized_resume_tokens = word_tokenize(resume_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of tokens in Resume: 424\n",
      "No. of tokens in Stemmed Resume: 363\n",
      "No. of tokens in Lemmatized Resume: 363\n"
     ]
    }
   ],
   "source": [
    "# Comparing the number of tokens in original, stemmed and lemmatized outputs\n",
    "print('No. of tokens in Resume:', len(resume_tokens))\n",
    "print('No. of tokens in Stemmed Resume:', len(stemmed_resume_tokens))\n",
    "print('No. of tokens in Lemmatized Resume:', len(lemmatized_resume_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that both the stemmed and lemmatized resume's have same number of tokesn which is correct as the tokenization step for both these processes uses the same Tokenization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No. of unique tokens/words in the stemmed output: 220\n",
      "No. of unique tokens/words in the lemmatized output: 229\n"
     ]
    }
   ],
   "source": [
    "# comparing no. of words and word frequencies in both stemmed and lemmatized outputs\n",
    "stemmed_resume_frequencies = Counter(stemmed_resume_tokens)\n",
    "lemmatized_resume_frequencies = Counter(lemmatized_resume_tokens)\n",
    "print('\\nNo. of unique tokens/words in the stemmed output:', len(stemmed_resume_frequencies))\n",
    "print('No. of unique tokens/words in the lemmatized output:', len(lemmatized_resume_frequencies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the stemmed output there are less number of tokens, but the reduction in number of tokens isn't that high and if the purpose of our task is to reduce the number of tokens in the corpus, stemming is definately a way to go, but lemmatization also achieves similar percentage reduction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 30 most common words/tokens in the stemmed output:\n",
      " [('and', 16), ('of', 12), ('work', 7), ('java', 7), ('the', 6), ('with', 5), ('on', 5), ('develop', 4), ('com', 4), ('delhi', 4), ('month', 4), ('to', 4), ('core', 4), ('data', 4), ('structur', 4), ('algorithm', 4), ('at', 4), ('univers', 4), ('cern', 4), ('in', 4), ('comput', 4), ('code', 3), ('creat', 3), ('teach', 3), ('auckland', 3), ('mathemat', 3), ('that', 3), ('applic', 3), ('http', 3), ('softwar', 2)]\n",
      "\n",
      "Top 30 most common words/tokens in the lemmatized output:\n",
      " [('and', 16), ('of', 12), ('java', 7), ('worked', 6), ('the', 6), ('with', 5), ('on', 5), ('com', 4), ('delhi', 4), ('month', 4), ('to', 4), ('core', 4), ('data', 4), ('structure', 4), ('algorithm', 4), ('at', 4), ('university', 4), ('cern', 4), ('in', 4), ('developer', 3), ('auckland', 3), ('mathematics', 3), ('that', 3), ('application', 3), ('computer', 3), ('http', 3), ('software', 2), ('new', 2), ('english', 2), ('4', 2)]\n"
     ]
    }
   ],
   "source": [
    "# seeing the top 30 most common words in the stemmed and lemmatized outputs\n",
    "print('\\nTop 30 most common words/tokens in the stemmed output:\\n', stemmed_resume_frequencies.most_common(30))\n",
    "print('\\nTop 30 most common words/tokens in the lemmatized output:\\n', lemmatized_resume_frequencies.most_common(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In stemming we are getting words reduced down to their roots and words are much more clearer in their meaning and are closer to their original form in the lemmatizaed format. Although in lemmatization we are also receiving a number __4__ in our most frequently ocurring characters.\n",
    "\n",
    "We now introduce a helpee method that will help us in tagging each token in the corpus with the corresponding Part of Speech Tags (POS tags). POS Tags are mainly of the following types:\n",
    "\n",
    "- Noun (n)\n",
    "- Verb (v)\n",
    "- Adjective (a)\n",
    "- Adverb (r)\n",
    "- Symbol (s)\n",
    "\n",
    "By tagging the orignal, stemmed and lemmatized resumes we can check whether we are still maintaing the same frequency of POS tags. Which will further show that the meaning or context of our words has been retained despite the pre-processing steps of Stemming or Lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resume POS Tags Frequency: Counter({'n': 202, 'v': 20, 'a': 18, 's': 10, 'r': 4})\n",
      "Stemmed Resume POS Tags Frequency: Counter({'n': 161, 'a': 11, 'v': 10, 's': 7, 'r': 4})\n",
      "Lemmatized Resume POS Tags Frequency: Counter({'n': 214, 'v': 20, 'a': 18, 's': 11, 'r': 5})\n"
     ]
    }
   ],
   "source": [
    "def get_pos_frequency(tokens: list) -> Counter:\n",
    "    synsets = [wordnet.synsets(token) for token in tokens]\n",
    "    pos_tags = []\n",
    "    for synset in synsets:\n",
    "        if isinstance(synset, list) and len(synset) > 0:\n",
    "            pos_tags.append(synset[0].pos())\n",
    "    return Counter(pos_tags)\n",
    "\n",
    "\n",
    "# Analyzing of frequency of POS tags in original, stemmed and Lemmatized resume\n",
    "resume_pos_frequency = get_pos_frequency(resume_tokens)\n",
    "stemmed_resume_pos_frequency = get_pos_frequency(stemmed_resume_tokens)\n",
    "lemmatized_resume_pos_frequency = get_pos_frequency(lemmatized_resume_tokens)\n",
    "\n",
    "print('\\nResume POS Tags Frequency:', resume_pos_frequency)\n",
    "print('Stemmed Resume POS Tags Frequency:', stemmed_resume_pos_frequency)\n",
    "print('Lemmatized Resume POS Tags Frequency:', lemmatized_resume_pos_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the number of nouns and adverbs increases in the resume after performing the lemmatization step and the number of nouns clearly decreases after performing stemming. So, if in our application the user wishes to search proper nouns and obtain specific results and only those that match exactly, like searching __java__, __python__ etc. through a resume, the lemmatization will give a better result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
